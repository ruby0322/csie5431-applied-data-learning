{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "VfE7ozBnPvMN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2WjOK-eyc-rZ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xeow-jHhYEQ-"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/huggingface/transformers.git\n",
        "# import os\n",
        "# os.chdir('transformers')\n",
        "# !pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hycAlJvd7yYK",
        "outputId": "4dee674b-b717-47f6-f54f-30b47dec1091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/accelerate\n",
            "  Cloning https://github.com/huggingface/accelerate to /tmp/pip-req-build-lrd9fxlj\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate /tmp/pip-req-build-lrd9fxlj\n",
            "  Resolved https://github.com/huggingface/accelerate to commit 127818fc27ebe5cb236357fff59ff1748326d643\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.35.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.35.0.dev0) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.35.0.dev0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.35.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.35.0.dev0) (2.4.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.35.0.dev0) (0.24.7)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.35.0.dev0) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.35.0.dev0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.35.0.dev0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.35.0.dev0) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.35.0.dev0) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.35.0.dev0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/huggingface/accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BZyNl6DI76dj"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate torch>=1.3 transformers accelerate>=0.12.0 sentencepiece!=0.1.92 protobuf tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olfBdGrN70UE"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-6_lZV88-F7",
        "outputId": "96b8c93c-093f-4f93-fce1-d2db1d5d2081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGQuhboR80KQ",
        "outputId": "81265fb0-fb0c-4f2c-bc4c-70be38b92f00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!touch ~/.kaggle/kaggle.json\n",
        "\n",
        "api_token = {\"username\":\"ruby0322\",\"key\":\"949a390a18b790a080fde3812391d784\"}\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpyI8akg8U0Q",
        "outputId": "b642e93d-56ff-43c3-9057-c1e2776f5c20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ntu-adl-2024-hw-1-chinese-extractive-qa.zip to /content\n",
            "\r  0% 0.00/7.64M [00:00<?, ?B/s]\n",
            "\r100% 7.64M/7.64M [00:00<00:00, 97.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c ntu-adl-2024-hw-1-chinese-extractive-qa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlVQ6zlR9X9a",
        "outputId": "93f9b6c6-12f8-430a-844b-663befe14061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ntu-adl-2024-hw-1-chinese-extractive-qa.zip\n",
            "  inflating: context.json            \n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test.json               \n",
            "  inflating: train.json              \n",
            "  inflating: valid.json              \n"
          ]
        }
      ],
      "source": [
        "!unzip ntu-adl-2024-hw-1-chinese-extractive-qa.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WoI4plyw9u92"
      },
      "outputs": [],
      "source": [
        "train = pd.read_json('train.json')\n",
        "train = train.drop(columns=['id'])\n",
        "valid = pd.read_json('valid.json')\n",
        "valid = valid.drop(columns=['id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pb0ZJd0WANfA"
      },
      "outputs": [],
      "source": [
        "contexts_df = pd.read_json('context.json')\n",
        "contexts = np.array(contexts_df[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xswCgiMtBiZf"
      },
      "outputs": [],
      "source": [
        "def transform_to_training_format(df, contexts):\n",
        "    formatted_data = []\n",
        "\n",
        "    # Loop over each row in the DataFrame\n",
        "    for index, row in df.iterrows():\n",
        "        question = row['question']\n",
        "        paragraphs = row['paragraphs']  # These are indices of contexts\n",
        "        relevant = row['relevant']\n",
        "\n",
        "        # Create the format\n",
        "        entry = {\n",
        "            \"sent1\": question,\n",
        "            \"ending0\": contexts[paragraphs[0]],\n",
        "            \"ending1\": contexts[paragraphs[1]],\n",
        "            \"ending2\": contexts[paragraphs[2]],\n",
        "            \"ending3\": contexts[paragraphs[3]],\n",
        "            \"label\": paragraphs.index(relevant)  # Find the index of the relevant context\n",
        "        }\n",
        "\n",
        "        # Append the formatted entry\n",
        "        formatted_data.append(entry)\n",
        "\n",
        "    return formatted_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MZX3_77yD3Bs"
      },
      "outputs": [],
      "source": [
        "train_data = transform_to_training_format(train, contexts)\n",
        "valid_data = transform_to_training_format(valid, contexts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-MG8pVUIau8y"
      },
      "outputs": [],
      "source": [
        "with open('train_data.json', 'w') as train_data_json:\n",
        "    json.dump(train_data, train_data_json)\n",
        "with open('valid_data.json', 'w') as valid_data_json:\n",
        "    json.dump(valid_data, valid_data_json)\n",
        "# valid_data.to_json(orient='records')\n",
        "# test_data.to_json(orient='records')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0uNkCcNi4Yg",
        "outputId": "647b0e38-8c24-4dca-c495-7e065687f323"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'csie5431-applied-data-learning'...\n",
            "remote: Enumerating objects: 78, done.\u001b[K\n",
            "remote: Counting objects: 100% (78/78), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 78 (delta 32), reused 73 (delta 27), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (78/78), 31.06 KiB | 1.24 MiB/s, done.\n",
            "Resolving deltas: 100% (32/32), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf ./csie5431-applied-data-learning\n",
        "!git clone https://github.com/ruby0322/csie5431-applied-data-learning.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPSlrAMoi-Us",
        "outputId": "3bd9f556-2564-49a0-ff76-f580b332a313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘./output’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mv ./csie5431-applied-data-learning/hw1/multiple_choice_train.py ./\n",
        "!mv ./csie5431-applied-data-learning/hw1/multiple_choice_inference.py ./\n",
        "!mv ./csie5431-applied-data-learning/hw1/multiple_choice_eval.py ./\n",
        "!mkdir ./output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "57crazrFGgpJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62f88e6a-bb1e-42f6-d303-6b20c3dd0748"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-10-08 13:48:31.949559: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-10-08 13:48:31.966987: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-08 13:48:31.988906: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-08 13:48:31.995405: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-08 13:48:32.011290: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-08 13:48:33.249126: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "10/08/2024 13:48:35 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: no\n",
            "\n",
            "Generating train split: 21714 examples [00:04, 4905.12 examples/s]\n",
            "Generating validation split: 3009 examples [00:00, 6056.39 examples/s]\n",
            "config.json: 100% 689/689 [00:00<00:00, 4.52MB/s]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--hfl--chinese-roberta-wwm-ext/snapshots/5c58d0b8ec1d9014354d691c538661bf00bfdb44/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"hfl/chinese-roberta-wwm-ext\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.46.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 21128\n",
            "}\n",
            "\n",
            "tokenizer_config.json: 100% 19.0/19.0 [00:00<00:00, 143kB/s]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--hfl--chinese-roberta-wwm-ext/snapshots/5c58d0b8ec1d9014354d691c538661bf00bfdb44/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"hfl/chinese-roberta-wwm-ext\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.46.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 21128\n",
            "}\n",
            "\n",
            "vocab.txt: 100% 110k/110k [00:00<00:00, 4.59MB/s]\n",
            "tokenizer.json: 100% 269k/269k [00:00<00:00, 2.96MB/s]\n",
            "added_tokens.json: 100% 2.00/2.00 [00:00<00:00, 14.7kB/s]\n",
            "special_tokens_map.json: 100% 112/112 [00:00<00:00, 787kB/s]\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--hfl--chinese-roberta-wwm-ext/snapshots/5c58d0b8ec1d9014354d691c538661bf00bfdb44/vocab.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--hfl--chinese-roberta-wwm-ext/snapshots/5c58d0b8ec1d9014354d691c538661bf00bfdb44/tokenizer.json\n",
            "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--hfl--chinese-roberta-wwm-ext/snapshots/5c58d0b8ec1d9014354d691c538661bf00bfdb44/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--hfl--chinese-roberta-wwm-ext/snapshots/5c58d0b8ec1d9014354d691c538661bf00bfdb44/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--hfl--chinese-roberta-wwm-ext/snapshots/5c58d0b8ec1d9014354d691c538661bf00bfdb44/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--hfl--chinese-roberta-wwm-ext/snapshots/5c58d0b8ec1d9014354d691c538661bf00bfdb44/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"hfl/chinese-roberta-wwm-ext\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.46.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 21128\n",
            "}\n",
            "\n",
            "pytorch_model.bin: 100% 412M/412M [00:02<00:00, 185MB/s]\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--hfl--chinese-roberta-wwm-ext/snapshots/5c58d0b8ec1d9014354d691c538661bf00bfdb44/pytorch_model.bin\n",
            "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertForMultipleChoice: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Map: 100% 21714/21714 [00:36<00:00, 595.12 examples/s]\n",
            "Map: 100% 3009/3009 [00:05<00:00, 601.11 examples/s]\n",
            "10/08/2024 13:49:25 - INFO - __main__ - Sample 20952 of the training set: {'input_ids': [[101, 1923, 782, 2152, 4638, 6956, 1146, 2456, 5064, 3297, 3193, 1377, 809, 6841, 3985, 5635, 1525, 671, 943, 3229, 3309, 136, 102, 868, 4158, 5958, 672, 2225, 765, 4374, 1751, 4638, 1184, 7674, 6963, 8024, 1762, 3336, 3360, 2356, 704, 2552, 1469, 1453, 6920, 1814, 7120, 3300, 2523, 1914, 4374, 2157, 2152, 3671, 510, 857, 2125, 1469, 1814, 1836, 511, 7370, 749, 8126, 686, 5145, 4158, 4374, 2157, 1923, 782, 1046, 7027, 3172, 5881, 2025, 185, 4454, 7927, 765, 934, 2456, 4638, 4374, 2152, 722, 1912, 8024, 6917, 3300, 1923, 782, 2152, 510, 1305, 7027, 2225, 765, 6330, 2152, 510, 4374, 1400, 1162, 1863, 1469, 4482, 961, 5881, 6330, 1814, 1836, 511, 8387, 2399, 8024, 3336, 3360, 809, 1350, 1453, 6920, 1814, 2356, 7027, 3753, 1164, 510, 5885, 1305, 1154, 7027, 510, 7500, 5152, 7027, 765, 7440, 765, 5844, 510, 7350, 1154, 510, 2861, 4906, 2225, 1395, 510, 3172, 1756, 4649, 2225, 1395, 510, 3797, 961, 858, 1469, 2762, 3753, 1058, 2792, 3075, 3300, 4638, 5958, 3753, 765, 4374, 2147, 2456, 5064, 8024, 6158, 5474, 1394, 1751, 3136, 4906, 3152, 5175, 5251, 6268, 4158, 686, 4518, 6909, 4496, 511, 3336, 3360, 6917, 3075, 3300, 3644, 1380, 2641, 719, 4638, 3490, 4289, 1754, 100, 3336, 3360, 3490, 4289, 1754, 511, 4294, 1162, 966, 2533, 671, 2990, 4638, 3221, 1923, 782, 2152, 8024, 679, 1006, 1006, 3221, 1728, 4158, 2124, 855, 3176, 3336, 3360, 4638, 3633, 704, 2552, 8024, 6917, 1728, 4158, 2124, 4638, 671, 6956, 1146, 1377, 809, 671, 4684, 6841, 3985, 1168, 1367, 5397, 7679, 3229, 3309, 511, 1762, 704, 686, 5145, 6858, 6882, 1872, 1217, 1369, 1912, 1060, 2429, 1849, 699, 1217, 7770, 7770, 2428, 4638, 3175, 2466, 8024, 2828, 2124, 6365, 2768, 749, 671, 2429, 1814, 1836, 511, 5445, 1168, 749, 8123, 686, 5145, 1159, 3309, 8024, 5838, 1164, 3797, 185, 2215, 4482, 2861, 4158, 1071, 2456, 6863, 749, 2168, 7927, 1828, 4640, 4638, 3633, 7481, 511, 5543, 1917, 935, 4748, 3146, 2429, 1814, 2356, 4638, 3221, 5979, 877, 4273, 1217, 1920, 3136, 1828, 8024, 2537, 6929, 6174, 1377, 809, 4692, 1168, 3336, 3360, 6158, 6895, 5993, 4948, 7434, 4648, 4648, 4638, 7350, 4273, 1292, 3172, 2255, 2792, 4472, 5254, 511, 3136, 1828, 678, 3175, 3221, 5958, 3753, 765, 2157, 3184, 1920, 1914, 3149, 2768, 1519, 4638, 1877, 1867, 511, 1377, 809, 6858, 6882, 5979, 877, 4273, 1217, 7971, 6724, 7136, 6662, 1168, 6888, 2255, 7515, 511, 1762, 1936, 4294, 3336, 3360, 6125, 1281, 3300, 2523, 1914, 3173, 5971, 6123, 6880, 1240, 2456, 5064, 8024, 6857, 886, 2533, 3336, 3360, 1469, 5101, 5984, 671, 6629, 8024, 2768, 4158, 6857, 4934, 7591, 3419, 1762, 5412, 1920, 1164, 4638, 704, 2552, 511, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1923, 782, 2152, 4638, 6956, 1146, 2456, 5064, 3297, 3193, 1377, 809, 6841, 3985, 5635, 1525, 671, 943, 3229, 3309, 136, 102, 2552, 4415, 2119, 3221, 1415, 4158, 5632, 4197, 4906, 2119, 4638, 5061, 1752, 8024, 4680, 1184, 738, 2213, 2100, 4261, 6359, 8024, 671, 5663, 6733, 2451, 4158, 2970, 1358, 4638, 6303, 3791, 3221, 2552, 4415, 2119, 1398, 3229, 1259, 1419, 1762, 5632, 4197, 4906, 2119, 5645, 4852, 3298, 4906, 2119, 4638, 5061, 4539, 722, 704, 511, 5632, 4197, 4906, 2119, 4638, 3418, 3315, 4680, 4638, 1762, 3176, 2204, 2823, 7403, 5966, 1762, 5632, 4197, 4412, 6496, 5520, 2527, 4638, 6211, 2526, 8024, 852, 3221, 5632, 4197, 4906, 2119, 4638, 2339, 868, 2213, 679, 1259, 2886, 4777, 4955, 4158, 784, 7938, 3298, 2100, 1762, 6857, 763, 6211, 2526, 511, 5632, 4197, 4906, 2119, 6291, 4158, 6631, 5632, 4197, 4638, 510, 7401, 2692, 4638, 1469, 5632, 4685, 4757, 4688, 4638, 4412, 6496, 3221, 679, 2100, 1762, 4638, 511, 5632, 4197, 4906, 2119, 4638, 3297, 7028, 6206, 4638, 1060, 943, 3118, 3393, 3221, 6223, 2175, 1469, 6922, 6744, 2972, 4415, 511, 4507, 2205, 5632, 4197, 4638, 6223, 2175, 1469, 6922, 6744, 2972, 4415, 5632, 4197, 4906, 2119, 1377, 809, 2471, 2206, 1139, 1920, 5632, 4197, 704, 4638, 6211, 2526, 511, 969, 1963, 6223, 2175, 4638, 4412, 6496, 5645, 6211, 2526, 4638, 7521, 6241, 679, 1398, 8024, 6929, 7938, 6206, 7938, 3221, 1728, 4158, 6223, 2175, 704, 3300, 7097, 6299, 8024, 6206, 7938, 3221, 1728, 4158, 5635, 3634, 4158, 3632, 6158, 6291, 4158, 3221, 3633, 4825, 4638, 6211, 2526, 3221, 7097, 6299, 4638, 511, 671, 943, 6631, 5632, 4197, 1728, 5162, 3221, 679, 2100, 1762, 4638, 511, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1923, 782, 2152, 4638, 6956, 1146, 2456, 5064, 3297, 3193, 1377, 809, 6841, 3985, 5635, 1525, 671, 943, 3229, 3309, 136, 102, 3644, 1380, 677, 3297, 3193, 6250, 6734, 6963, 3377, 3360, 4638, 3221, 1062, 1039, 8468, 2399, 8024, 2361, 5626, 1921, 3152, 2119, 2157, 1469, 1765, 1756, 2119, 2157, 2805, 1239, 2166, 8024, 4534, 3229, 800, 4935, 6963, 3377, 3360, 4158, 2695, 2357, 2861, 5152, 511, 6963, 3377, 3360, 3297, 3193, 4638, 2233, 3696, 1377, 809, 6841, 3985, 1168, 1062, 1039, 1184, 671, 686, 5145, 8039, 6963, 3377, 3360, 1762, 13180, 2399, 2340, 1381, 2456, 4989, 511, 6857, 1060, 943, 1814, 7120, 3297, 5173, 6084, 1394, 2768, 671, 943, 1814, 7120, 511, 4412, 1762, 4638, 1814, 2356, 793, 4197, 924, 4522, 749, 1184, 1288, 6956, 1146, 4638, 5739, 6295, 1265, 4638, 2695, 4273, 5984, 1399, 2099, 1469, 2527, 1288, 6956, 1146, 4638, 5155, 2695, 4273, 5984, 1399, 2099, 511, 1762, 6330, 3294, 5018, 782, 1057, 909, 2695, 4273, 5984, 722, 2527, 8024, 6963, 3377, 3360, 912, 1357, 807, 749, 1849, 2861, 687, 2768, 4158, 2695, 4273, 5984, 4638, 7674, 6963, 511, 2537, 1062, 1039, 8122, 686, 5145, 671, 4684, 1168, 8121, 686, 5145, 3314, 3309, 8024, 6963, 3377, 3360, 1469, 1071, 7353, 6818, 1765, 1281, 100, 100, 6158, 4935, 4158, 2364, 5844, 100, 100, 3221, 5739, 1751, 782, 1762, 2695, 4273, 5984, 1546, 671, 2971, 1169, 4638, 1765, 1281, 511, 2537, 8126, 686, 5145, 7274, 1993, 8024, 1814, 2356, 1762, 2184, 7295, 6125, 6887, 752, 1243, 1999, 1519, 3298, 4638, 2396, 1221, 678, 7274, 1993, 6813, 6862, 3097, 2484, 511, 1605, 3780, 765, 6963, 3377, 3360, 3295, 671, 2428, 3221, 1920, 5739, 2370, 1751, 1006, 3613, 3176, 961, 3142, 4638, 5018, 753, 1920, 1814, 2356, 511, 2523, 1914, 6963, 3377, 3360, 4638, 1032, 4899, 2456, 5064, 6963, 3221, 1762, 6857, 671, 3229, 3309, 2456, 4989, 4638, 511, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1923, 782, 2152, 4638, 6956, 1146, 2456, 5064, 3297, 3193, 1377, 809, 6841, 3985, 5635, 1525, 671, 943, 3229, 3309, 136, 102, 4412, 807, 3229, 3309, 4638, 7274, 1993, 807, 6134, 749, 3627, 3828, 3152, 5971, 2541, 5646, 4638, 5178, 3338, 511, 4825, 1147, 4638, 3229, 7279, 7953, 3298, 898, 3087, 1392, 943, 7526, 1818, 4638, 4518, 2137, 5445, 3300, 2792, 679, 1398, 8024, 3683, 1963, 6303, 8024, 671, 943, 3644, 1380, 2119, 2157, 1377, 5543, 3298, 2200, 4412, 807, 4638, 7274, 1993, 3123, 1762, 9316, 8129, 2399, 8024, 5445, 671, 943, 7509, 3556, 2157, 1179, 1377, 5543, 3298, 2828, 3229, 7279, 3123, 1762, 7509, 3556, 4638, 3857, 4035, 3229, 3309, 5178, 3338, 722, 2527, 8024, 738, 2218, 3221, 1920, 5147, 8985, 2399, 511, 4534, 807, 1380, 2119, 5442, 5424, 2715, 2828, 671, 2782, 4255, 4634, 8024, 868, 4158, 6818, 807, 4638, 5173, 5178, 510, 4412, 807, 4638, 7274, 4999, 511, 1398, 3564, 1765, 8024, 898, 3087, 1392, 943, 7526, 1818, 4638, 679, 1398, 6223, 7953, 8024, 4412, 807, 3229, 3309, 1377, 5543, 3298, 6158, 4518, 2137, 4158, 671, 4684, 2454, 847, 1168, 2769, 947, 4412, 1762, 2792, 5993, 4638, 3229, 7279, 8024, 2772, 5442, 738, 3300, 1377, 5543, 6158, 6291, 4158, 5178, 3338, 3176, 2527, 4412, 807, 712, 5412, 4638, 7274, 1993, 8024, 1377, 5543, 1762, 8779, 2399, 807, 1168, 8499, 2399, 807, 1159, 3309, 722, 7279, 4638, 818, 862, 671, 7953, 511, 1963, 3362, 3221, 2200, 4412, 807, 2137, 5412, 4158, 1762, 2527, 4412, 807, 722, 1184, 4638, 6282, 8024, 6929, 7938, 2124, 1377, 5543, 4294, 1162, 2900, 3868, 749, 4412, 807, 712, 5412, 511, 3300, 4638, 6223, 7953, 1179, 6291, 4158, 8024, 2527, 4412, 807, 712, 5412, 1372, 679, 6882, 3221, 4412, 807, 712, 5412, 3315, 6716, 3297, 3241, 671, 943, 3229, 3309, 4638, 4634, 2245, 5445, 2347, 511, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'labels': 0}.\n",
            "10/08/2024 13:49:25 - INFO - __main__ - Sample 3648 of the training set: {'input_ids': [[101, 5018, 753, 1282, 673, 2234, 6631, 5159, 4656, 6794, 1762, 1525, 671, 2399, 136, 102, 8516, 2399, 122, 3299, 8024, 989, 1046, 3481, 1762, 5018, 753, 1282, 673, 2234, 6631, 5159, 4656, 4638, 704, 1842, 828, 2622, 3229, 3667, 4412, 6716, 511, 1728, 4158, 2407, 2399, 809, 889, 782, 947, 2205, 704, 1842, 828, 2622, 4638, 5646, 6637, 7076, 3938, 8024, 1751, 2157, 3576, 3611, 4413, 5474, 4673, 3748, 2137, 2204, 3724, 1920, 4277, 3209, 3215, 889, 1429, 2471, 6223, 4707, 699, 2990, 7770, 3119, 6213, 4372, 8024, 989, 1046, 3481, 1728, 4158, 800, 4638, 4761, 1399, 2428, 1469, 1429, 2471, 1213, 5445, 1057, 6908, 511, 6857, 738, 3221, 6631, 5159, 4656, 7674, 3613, 1762, 704, 1842, 828, 2622, 4638, 3119, 6213, 4372, 7770, 3176, 3683, 6555, 3315, 6716, 511, 6134, 4028, 671, 7274, 1993, 8024, 989, 1046, 3481, 2537, 5659, 1378, 678, 3175, 6663, 1139, 8024, 5645, 3634, 1398, 3229, 800, 6716, 2527, 4638, 4195, 4125, 5211, 3123, 7274, 889, 511, 800, 671, 1240, 679, 1240, 4638, 4991, 5865, 8024, 519, 5215, 2995, 5865, 2891, 7531, 8024, 4348, 1963, 7425, 1008, 4638, 2013, 2706, 520, 8024, 6716, 4959, 7032, 5682, 1469, 7946, 5682, 4685, 7279, 6132, 3302, 8024, 7531, 2785, 1922, 7382, 7128, 511, 800, 924, 2898, 671, 1240, 679, 1240, 4638, 2013, 2706, 7269, 6888, 671, 1146, 1288, 7132, 8024, 6223, 4707, 2898, 5265, 4638, 3631, 1461, 511, 800, 4197, 2527, 5227, 5227, 1765, 3036, 678, 1922, 7382, 7128, 8024, 2803, 1403, 782, 5408, 8024, 4028, 1548, 749, 1724, 7674, 3625, 3289, 8038, 517, 1737, 1862, 518, 510, 517, 3683, 1164, 185, 5080, 518, 510, 517, 7946, 2772, 4635, 518, 1469, 517, 3780, 4618, 686, 4518, 518, 511, 989, 1046, 3481, 4638, 2201, 6744, 517, 1314, 7402, 722, 3180, 518, 1762, 2961, 6121, 3528, 677, 677, 1285, 749, 8192, 855, 511, 989, 1046, 3481, 1762, 5018, 8198, 2234, 5867, 5844, 5401, 4354, 677, 6158, 2956, 750, 749, 5867, 5844, 5401, 1001, 1936, 4354, 8024, 517, 7946, 2772, 4635, 518, 4363, 2533, 749, 519, 3297, 881, 5476, 3556, 6134, 4412, 4354, 520, 4638, 2990, 1399, 8024, 517, 1737, 1862, 518, 4363, 2533, 749, 519, 3297, 881, 160, 111, 144, 5476, 3556, 6134, 4412, 4354, 520, 1469, 519, 3297, 881, 160, 111, 144, 3625, 3289, 520, 4638, 2990, 1399, 511, 2201, 6744, 517, 1314, 7402, 722, 3180, 518, 4363, 2533, 749, 519, 7478, 1367, 1073, 7546, 3297, 881, 5032, 1205, 4354, 520, 8024, 5965, 6310, 185, 3172, 5204, 7524, 1469, 3805, 6832, 185, 5844, 1164, 4363, 2533, 749, 3634, 4354, 7517, 511, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 5018, 753, 1282, 673, 2234, 6631, 5159, 4656, 6794, 1762, 1525, 671, 2399, 136, 102, 9027, 2399, 4638, 1094, 6725, 3221, 6205, 2548, 8024, 8182, 2399, 4638, 1094, 6725, 3221, 6205, 4408, 4280, 8024, 6857, 1060, 943, 4413, 7386, 3221, 3644, 1380, 677, 123, 943, 1762, 1954, 2533, 3627, 3828, 1751, 2157, 4656, 722, 2527, 8024, 2970, 6865, 1086, 6560, 2533, 686, 4518, 4656, 4638, 4413, 7386, 511, 1762, 8508, 2399, 3627, 3828, 1751, 2157, 4656, 5647, 6121, 1184, 8024, 1298, 3172, 2861, 1923, 4255, 4634, 1058, 2782, 8024, 6283, 1751, 6158, 4881, 3632, 1347, 6555, 8024, 4507, 710, 7930, 6171, 677, 8024, 699, 809, 519, 7946, 7679, 520, 2013, 2706, 6355, 2183, 519, 710, 7930, 4997, 6282, 520, 8024, 1954, 2533, 1094, 6725, 511, 2768, 4264, 7674, 3118, 809, 519, 7946, 7679, 520, 2013, 2706, 1954, 1094, 4638, 4413, 7386, 511, 8202, 2399, 5647, 6121, 4638, 3627, 3828, 1751, 2157, 4656, 8024, 7674, 3613, 4507, 1060, 943, 1751, 2157, 3683, 1164, 3229, 1350, 5792, 5984, 1066, 1398, 712, 6794, 8024, 3297, 2527, 1094, 6725, 4507, 3791, 1751, 1954, 2533, 511, 8202, 2399, 4638, 1094, 6725, 3221, 3791, 1751, 8024, 8151, 2399, 4638, 1094, 6725, 3221, 6205, 4408, 4280, 8024, 6857, 1060, 943, 4413, 7386, 3221, 3644, 1380, 677, 123, 943, 1762, 1954, 2533, 686, 4518, 4656, 722, 2527, 8024, 2970, 6865, 1086, 6560, 2533, 3627, 3828, 1751, 2157, 4656, 4638, 4413, 7386, 511, 6205, 2548, 1350, 4412, 791, 4638, 2548, 1751, 3295, 1044, 2527, 129, 3613, 3231, 6716, 3627, 3828, 1751, 2157, 4656, 3976, 3748, 6555, 8024, 4534, 704, 127, 3613, 7300, 1057, 3748, 6555, 8024, 3297, 2527, 671, 1066, 676, 3613, 1954, 1094, 5445, 1726, 8024, 3221, 5635, 791, 2768, 5245, 3297, 1962, 4638, 4413, 7386, 511, 8258, 2399, 5647, 6121, 4638, 3627, 3828, 1751, 2157, 4656, 704, 8024, 2361, 5626, 809, 519, 7946, 7679, 520, 2013, 2706, 6355, 2183, 519, 2361, 5626, 4868, 6282, 520, 8024, 2768, 4158, 8258, 2399, 4638, 3627, 3828, 7464, 712, 511, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 5018, 753, 1282, 673, 2234, 6631, 5159, 4656, 6794, 1762, 1525, 671, 2399, 136, 102, 2876, 785, 2710, 2225, 7946, 1762, 2768, 4989, 722, 1184, 3295, 3221, 2710, 2225, 7946, 4511, 2094, 7768, 3082, 936, 3556, 6956, 4638, 6639, 4413, 6956, 7271, 511, 8985, 2399, 123, 3299, 8149, 3189, 8024, 6283, 936, 3556, 6956, 1374, 7274, 1059, 7768, 3298, 6359, 1415, 3748, 749, 6639, 4413, 6956, 2361, 3307, 1217, 1057, 2548, 1751, 6639, 4413, 1295, 3298, 4638, 6313, 3724, 8024, 6639, 4413, 6956, 4638, 8111, 1399, 4413, 1519, 7401, 1315, 6842, 1139, 3298, 6359, 8024, 699, 3176, 4534, 3241, 1762, 2472, 3306, 5754, 185, 5147, 5432, 4638, 2380, 7526, 678, 1201, 4989, 749, 2876, 785, 2710, 2225, 7946, 6639, 4413, 936, 3556, 6956, 511, 1762, 4764, 4764, 4638, 2407, 943, 3299, 1058, 8024, 2876, 785, 912, 809, 1920, 3683, 3149, 2782, 1245, 749, 2792, 3300, 3315, 1765, 5000, 4261, 2205, 2797, 8024, 671, 5647, 3669, 1057, 749, 8985, 118, 8146, 4413, 2108, 4638, 1298, 2548, 6639, 4413, 7093, 3560, 6555, 3976, 3748, 6555, 511, 1762, 2970, 678, 889, 4638, 2407, 2399, 704, 8024, 2876, 785, 4363, 2533, 749, 1914, 943, 4534, 1765, 5474, 6555, 4638, 1094, 6725, 4354, 4656, 8024, 699, 1762, 10428, 118, 8111, 4413, 2108, 1217, 1057, 749, 3173, 2768, 4989, 4638, 5238, 5159, 5474, 6555, 8024, 6857, 738, 3221, 2349, 827, 1164, 765, 2336, 4638, 7674, 943, 1281, 1818, 2595, 5474, 6555, 511, 2876, 785, 4363, 2533, 749, 5474, 6555, 1201, 6794, 1159, 2399, 4638, 1094, 6725, 8024, 852, 1320, 4192, 3791, 1086, 2428, 1954, 1094, 8024, 4684, 5635, 9837, 2399, 5018, 671, 3613, 686, 4518, 1920, 2782, 7274, 1993, 2527, 8024, 2548, 1751, 977, 3632, 749, 2792, 3300, 6639, 4413, 3833, 1240, 511, 671, 2782, 5178, 3338, 2527, 4638, 2407, 2399, 704, 8024, 2876, 785, 2710, 2225, 7946, 4363, 2533, 749, 2407, 943, 1281, 1818, 2595, 4638, 6555, 752, 1094, 6725, 511, 10126, 2399, 8024, 4413, 7386, 1954, 2533, 749, 3644, 1380, 677, 5018, 671, 943, 1298, 2548, 1094, 6725, 8024, 1060, 2399, 2527, 8024, 800, 947, 1086, 3613, 6185, 6182, 749, 6857, 671, 2768, 2218, 511, 9737, 2399, 8024, 2876, 785, 7674, 3613, 1762, 1751, 2157, 5159, 6555, 752, 704, 4633, 7515, 8024, 4534, 3229, 4638, 3136, 5230, 4415, 2175, 185, 4906, 2617, 4372, 7386, 809, 123, 131, 121, 3080, 3134, 3791, 5984, 1046, 4886, 5445, 4363, 2533, 749, 2548, 1751, 6639, 4413, 1094, 6725, 511, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 5018, 753, 1282, 673, 2234, 6631, 5159, 4656, 6794, 1762, 1525, 671, 2399, 136, 102, 5867, 5844, 5401, 4354, 4354, 4656, 3221, 671, 2429, 2207, 1798, 4638, 7103, 7032, 4522, 5476, 3582, 1848, 1008, 8024, 1728, 149, 9661, 9133, 10271, 3273, 4935, 149, 9661, 9009, 8024, 3125, 4935, 5867, 5844, 5401, 4354, 511, 5867, 5844, 5401, 4354, 3891, 5901, 4638, 4354, 7517, 4685, 4534, 2451, 3793, 8024, 1259, 2886, 3837, 6121, 510, 7442, 7509, 510, 3015, 4020, 510, 4265, 1894, 510, 1367, 1073, 510, 3696, 6343, 510, 4886, 7509, 5023, 7509, 3556, 7546, 1798, 8024, 3221, 4507, 3511, 1058, 782, 1894, 8024, 738, 2218, 3221, 7087, 7509, 2119, 7368, 4638, 2768, 1519, 2832, 4873, 3748, 2137, 8024, 5445, 679, 3221, 1008, 1440, 4850, 4277, 7509, 3556, 4354, 2772, 1059, 5401, 7509, 3556, 4354, 6929, 3564, 4507, 782, 3706, 3748, 2137, 511, 4507, 855, 3176, 4906, 5397, 2861, 1914, 2336, 4638, 3683, 7470, 3172, 2339, 5971, 1062, 1385, 6182, 868, 511, 7523, 4354, 1073, 4891, 677, 2128, 2961, 4761, 1399, 5971, 782, 4412, 1842, 4028, 1139, 8024, 699, 6858, 6882, 7442, 6213, 6752, 3064, 3176, 1059, 1751, 3064, 136, 511, 5867, 5844, 5401, 4354, 3176, 1282, 3299, 671, 3189, 7274, 1993, 1841, 1399, 8024, 1728, 3634, 3625, 2797, 947, 1762, 736, 3299, 2340, 1381, 6963, 5160, 5160, 2972, 136, 868, 1501, 8024, 809, 912, 4363, 2533, 1347, 6908, 5422, 2399, 5867, 5844, 5401, 4354, 4638, 6536, 3419, 511, 1963, 8038, 5147, 5432, 185, 5965, 1022, 1469, 2207, 7029, 3817, 2094, 4638, 2201, 6744, 11255, 12436, 3221, 8499, 2399, 8111, 3299, 2972, 136, 4638, 8024, 2792, 809, 7097, 6882, 8785, 2399, 4638, 3419, 5844, 5401, 4354, 8024, 852, 6560, 2533, 8687, 2399, 4638, 2399, 2428, 2201, 6744, 4354, 511, 5867, 5844, 5401, 4354, 4638, 6268, 6908, 3300, 671, 1947, 4685, 4534, 1713, 3419, 4638, 1169, 2428, 1469, 4923, 2415, 8024, 852, 3221, 1963, 1398, 1071, 2124, 1751, 7396, 5159, 4638, 7523, 4354, 1073, 4891, 8024, 5867, 5844, 5401, 4354, 738, 3295, 4634, 4495, 671, 943, 4354, 7517, 3300, 1060, 855, 2533, 712, 4638, 2658, 2501, 8024, 1963, 8555, 2399, 5018, 8229, 2234, 4638, 3297, 881, 5059, 1941, 5965, 6310, 1957, 3625, 2797, 2218, 4507, 5439, 2200, 877, 5881, 185, 2861, 6509, 1051, 1469, 3173, 4899, 7927, 5801, 185, 6527, 7434, 1060, 782, 1066, 1398, 2533, 4354, 511, 2779, 5635, 8127, 2399, 8024, 5867, 5844, 5401, 4354, 1073, 4891, 4680, 1184, 4507, 1520, 961, 3683, 765, 2451, 3064, 1062, 1385, 13035, 6511, 6519, 6752, 3064, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'labels': 0}.\n",
            "10/08/2024 13:49:26 - INFO - __main__ - Sample 819 of the training set: {'input_ids': [[101, 7797, 7546, 5645, 1060, 3483, 7546, 4638, 1058, 5606, 8024, 4935, 4158, 784, 7938, 136, 102, 5399, 5606, 1240, 4289, 4638, 2137, 5412, 4158, 5524, 5522, 4634, 2245, 1139, 2407, 2251, 1920, 7481, 4948, 4638, 5606, 8024, 1259, 2886, 8038, 5399, 5606, 510, 5184, 3688, 5606, 510, 2228, 5606, 511, 5524, 5522, 4684, 2970, 2768, 7269, 4158, 3300, 1724, 5501, 5645, 1914, 2251, 1331, 677, 4649, 4638, 7380, 4495, 2501, 2466, 8024, 5445, 7478, 1060, 3483, 7546, 6929, 4934, 7674, 1044, 4158, 6069, 6020, 2405, 7768, 1086, 2130, 1059, 6365, 7768, 4638, 3175, 2466, 511, 1762, 5399, 5606, 1240, 4289, 6174, 8024, 1060, 2251, 6134, 4649, 6752, 6365, 1168, 6235, 6549, 677, 4649, 8024, 3221, 4507, 5524, 5522, 3229, 3309, 4638, 4508, 4311, 5593, 6531, 4273, 5885, 889, 6863, 2768, 8024, 5445, 7478, 2130, 1059, 6365, 2501, 511, 5399, 5606, 1240, 4289, 4638, 4360, 4294, 5524, 5522, 4294, 2547, 6863, 2768, 6028, 4638, 4294, 1265, 8024, 1377, 809, 1762, 3291, 746, 4246, 4638, 4472, 1862, 4495, 2100, 8024, 6917, 3300, 6028, 6174, 1920, 7768, 4948, 4638, 6028, 7941, 511, 886, 5399, 5606, 1240, 4289, 1377, 1762, 7380, 1765, 677, 4495, 2100, 4638, 4294, 2547, 8024, 1259, 2886, 1830, 1743, 4638, 6851, 3706, 4649, 6549, 2772, 1830, 4801, 4638, 6028, 8024, 6917, 3300, 914, 6868, 1461, 1429, 5645, 2990, 897, 2450, 4289, 5993, 4415, 4638, 2228, 5606, 511, 4284, 947, 4638, 5575, 5627, 5645, 1920, 5591, 6900, 1394, 924, 2898, 3717, 1146, 511, 1920, 6956, 1146, 1529, 745, 7546, 699, 679, 4684, 2970, 4496, 1317, 8024, 5445, 4634, 2245, 1139, 5522, 4676, 5023, 5178, 3539, 511, 5018, 671, 943, 5399, 5606, 1240, 4289, 3221, 8850, 9182, 8957, 8321, 153, 8601, 9172, 8024, 4495, 2100, 3176, 124, 1023, 8442, 5857, 2399, 1184, 8024, 2253, 3176, 4260, 6121, 2501, 7546, 8024, 1912, 6134, 7546, 849, 2207, 1798, 6060, 6062, 511, 4284, 947, 4638, 6028, 2523, 2207, 684, 6208, 5901, 5442, 5606, 8024, 3760, 3300, 1008, 4412, 807, 5399, 5606, 1240, 4289, 6929, 3564, 1830, 4801, 4638, 3670, 511, 7426, 4197, 3378, 763, 4412, 807, 1060, 3483, 7546, 1762, 7380, 1765, 677, 4496, 1317, 8024, 2124, 947, 5375, 726, 3291, 6868, 1265, 4638, 4294, 2547, 1963, 5399, 5606, 8024, 3760, 3300, 4294, 1162, 924, 6362, 8024, 6857, 3564, 4638, 1317, 6858, 2382, 3221, 7768, 1912, 1358, 5125, 4638, 5178, 3362, 511, 5399, 5606, 1240, 4289, 4028, 1265, 1139, 1912, 6956, 4638, 5606, 8024, 3382, 6727, 4638, 1912, 3670, 8024, 809, 2205, 2834, 7380, 1765, 677, 1713, 1341, 4638, 4472, 1862, 8024, 1377, 809, 4684, 2970, 4496, 1317, 1762, 7380, 1765, 677, 8024, 3683, 4496, 1317, 1762, 3717, 704, 2128, 1059, 511, 5399, 5606, 1240, 4289, 4638, 4862, 1044, 4496, 1317, 1762, 4060, 4086, 4638, 4472, 1862, 8024, 6857, 3564, 6900, 2428, 1920, 2207, 4638, 1240, 4289, 679, 4500, 7937, 4214, 1765, 2204, 2823, 3990, 1765, 2772, 5442, 1367, 807, 3481, 3360, 704, 4638, 1394, 6900, 1765, 7953, 809, 4496, 1317, 8024, 746, 4246, 4638, 4472, 1862, 1377, 5543, 679, 3221, 6727, 1912, 3670, 1139, 4412, 4638, 712, 6206, 1333, 1728, 511, 102, 0, 0, 0], [101, 7797, 7546, 5645, 1060, 3483, 7546, 4638, 1058, 5606, 8024, 4935, 4158, 784, 7938, 136, 102, 8420, 8160, 2399, 8024, 3791, 1751, 1300, 4289, 2119, 2157, 2861, 7679, 1046, 2990, 1139, 8038, 519, 2792, 3300, 4495, 4289, 7768, 6963, 4507, 5169, 5528, 2792, 5175, 2768, 8024, 5169, 5528, 6174, 7481, 6963, 1419, 3300, 763, 3298, 3837, 1240, 4638, 521, 3890, 7768, 522, 511, 520, 1320, 3760, 3300, 1072, 7768, 4638, 6223, 2175, 6349, 3087, 3118, 2898, 6857, 943, 6303, 3791, 511, 9952, 8159, 2399, 8024, 3791, 1751, 3490, 4289, 2119, 2157, 3336, 2805, 2622, 1762, 6316, 3152, 704, 2990, 1139, 519, 5169, 5528, 4825, 2179, 3221, 4495, 4289, 7768, 4638, 1825, 3315, 3539, 6863, 520, 1348, 1728, 4158, 3490, 4289, 5169, 5528, 3683, 1240, 4289, 5169, 5528, 1914, 749, 5169, 5528, 1880, 8024, 1728, 3634, 6223, 2175, 2825, 6123, 6917, 679, 2768, 4225, 4638, 3229, 952, 3683, 1240, 4289, 5169, 5528, 3291, 2159, 3211, 6223, 2175, 8024, 738, 1728, 3634, 6857, 943, 6303, 3791, 1044, 6158, 3490, 4289, 2119, 5442, 2970, 1358, 511, 8131, 686, 5145, 704, 3309, 8024, 2548, 1751, 1240, 4289, 2119, 2157, 6258, 3200, 6868, 671, 3635, 4634, 4412, 1240, 4289, 5169, 5528, 6174, 3300, 5169, 5528, 3417, 8024, 3417, 4638, 1453, 1752, 3300, 3890, 4311, 4289, 6549, 8024, 1762, 1912, 1750, 6917, 3300, 671, 2251, 5606, 8024, 1320, 3760, 3300, 5169, 5528, 1880, 8024, 800, 6291, 4158, 5169, 5528, 4638, 712, 6206, 6956, 1146, 3221, 5169, 5528, 3417, 5445, 7478, 1912, 1750, 4638, 5169, 5528, 1880, 511, 868, 4158, 7015, 4495, 4638, 2113, 3152, 1179, 2828, 5169, 5528, 6358, 868, 519, 4495, 1039, 520, 511, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 7797, 7546, 5645, 1060, 3483, 7546, 4638, 1058, 5606, 8024, 4935, 4158, 784, 7938, 136, 102, 7797, 7546, 5645, 1060, 3483, 7546, 1372, 3300, 671, 2251, 1058, 5606, 8024, 738, 4935, 4158, 5524, 5606, 511, 5399, 5606, 1240, 4289, 4638, 1317, 1058, 6956, 5178, 3539, 3291, 1217, 6868, 1265, 8024, 3173, 4028, 1265, 1139, 4638, 5178, 3539, 1377, 897, 1317, 1762, 5524, 5522, 769, 2994, 4958, 3706, 8024, 1398, 3229, 738, 1377, 5993, 4415, 2450, 4289, 511, 4158, 749, 3291, 1331, 510, 3291, 1830, 4801, 4638, 3670, 8024, 1372, 7479, 3097, 3141, 868, 4500, 1377, 5543, 679, 6639, 1917, 8024, 1372, 3300, 809, 3173, 3175, 3791, 2990, 897, 5524, 5522, 3709, 3706, 511, 1762, 1317, 3300, 749, 6857, 763, 5178, 3539, 722, 2527, 8024, 3291, 6185, 7429, 4638, 1317, 1377, 6366, 5399, 5606, 1240, 4289, 1762, 3291, 746, 4246, 4638, 4472, 1862, 4496, 678, 3291, 1920, 4638, 1317, 511, 3291, 1920, 4638, 1317, 2692, 1456, 5442, 3291, 1920, 4638, 2405, 2399, 7768, 8024, 5445, 3291, 1920, 4638, 2768, 2399, 7768, 1377, 4496, 678, 3291, 1920, 4638, 1317, 8024, 2692, 1456, 5442, 5399, 5606, 1240, 4289, 1377, 3683, 4284, 947, 4638, 4862, 1044, 3291, 861, 1032, 1248, 8024, 7269, 2533, 3291, 1920, 511, 852, 4684, 1168, 4284, 947, 977, 3632, 2537, 2207, 1798, 4192, 5550, 3491, 1240, 4289, 3109, 1357, 7608, 4289, 8024, 699, 684, 7274, 1993, 809, 3490, 4289, 5645, 1071, 800, 5550, 3491, 1240, 4289, 6868, 7608, 8024, 2772, 5442, 7028, 3173, 6819, 1726, 3717, 6174, 8024, 2798, 2768, 4158, 4696, 3633, 4638, 1032, 1248, 1240, 4289, 511, 3173, 4638, 5424, 2595, 5645, 3291, 7028, 4638, 6716, 7768, 2692, 1456, 5442, 5399, 5606, 1240, 4289, 1762, 6121, 4158, 5645, 4495, 4415, 5178, 3539, 677, 3291, 1217, 6868, 1265, 511, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 7797, 7546, 5645, 1060, 3483, 7546, 4638, 1058, 5606, 8024, 4935, 4158, 784, 7938, 136, 102, 4412, 807, 3229, 3309, 4638, 7274, 1993, 807, 6134, 749, 3627, 3828, 3152, 5971, 2541, 5646, 4638, 5178, 3338, 511, 4825, 1147, 4638, 3229, 7279, 7953, 3298, 898, 3087, 1392, 943, 7526, 1818, 4638, 4518, 2137, 5445, 3300, 2792, 679, 1398, 8024, 3683, 1963, 6303, 8024, 671, 943, 3644, 1380, 2119, 2157, 1377, 5543, 3298, 2200, 4412, 807, 4638, 7274, 1993, 3123, 1762, 9316, 8129, 2399, 8024, 5445, 671, 943, 7509, 3556, 2157, 1179, 1377, 5543, 3298, 2828, 3229, 7279, 3123, 1762, 7509, 3556, 4638, 3857, 4035, 3229, 3309, 5178, 3338, 722, 2527, 8024, 738, 2218, 3221, 1920, 5147, 8985, 2399, 511, 4534, 807, 1380, 2119, 5442, 5424, 2715, 2828, 671, 2782, 4255, 4634, 8024, 868, 4158, 6818, 807, 4638, 5173, 5178, 510, 4412, 807, 4638, 7274, 4999, 511, 1398, 3564, 1765, 8024, 898, 3087, 1392, 943, 7526, 1818, 4638, 679, 1398, 6223, 7953, 8024, 4412, 807, 3229, 3309, 1377, 5543, 3298, 6158, 4518, 2137, 4158, 671, 4684, 2454, 847, 1168, 2769, 947, 4412, 1762, 2792, 5993, 4638, 3229, 7279, 8024, 2772, 5442, 738, 3300, 1377, 5543, 6158, 6291, 4158, 5178, 3338, 3176, 2527, 4412, 807, 712, 5412, 4638, 7274, 1993, 8024, 1377, 5543, 1762, 8779, 2399, 807, 1168, 8499, 2399, 807, 1159, 3309, 722, 7279, 4638, 818, 862, 671, 7953, 511, 1963, 3362, 3221, 2200, 4412, 807, 2137, 5412, 4158, 1762, 2527, 4412, 807, 722, 1184, 4638, 6282, 8024, 6929, 7938, 2124, 1377, 5543, 4294, 1162, 2900, 3868, 749, 4412, 807, 712, 5412, 511, 3300, 4638, 6223, 7953, 1179, 6291, 4158, 8024, 2527, 4412, 807, 712, 5412, 1372, 679, 6882, 3221, 4412, 807, 712, 5412, 3315, 6716, 3297, 3241, 671, 943, 3229, 3309, 4638, 4634, 2245, 5445, 2347, 511, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'labels': 2}.\n",
            "10/08/2024 13:49:26 - INFO - __main__ - ***** Running training *****\n",
            "10/08/2024 13:49:26 - INFO - __main__ -   Num examples = 21714\n",
            "10/08/2024 13:49:26 - INFO - __main__ -   Num Epochs = 3\n",
            "10/08/2024 13:49:26 - INFO - __main__ -   Instantaneous batch size per device = 4\n",
            "10/08/2024 13:49:26 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "10/08/2024 13:49:26 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
            "10/08/2024 13:49:26 - INFO - __main__ -   Total optimization steps = 16287\n",
            " 33% 5429/16287 [28:40<56:25,  3.21it/s]epoch 0: {'accuracy': 0.9561316051844466}\n",
            " 67% 10858/16287 [58:42<28:11,  3.21it/s]epoch 1: {'accuracy': 0.9328680624792289}\n",
            "100% 16287/16287 [1:28:44<00:00,  3.22it/s]epoch 2: {'accuracy': 0.9587902957793287}\n",
            "Configuration saved in ./output/config.json\n",
            "Model weights saved in ./output/model.safetensors\n",
            "tokenizer config file saved in ./output/tokenizer_config.json\n",
            "Special tokens file saved in ./output/special_tokens_map.json\n",
            "100% 16287/16287 [1:30:09<00:00,  3.01it/s]\n"
          ]
        }
      ],
      "source": [
        "!python multiple_choice_train.py \\\n",
        "  --train_file train_data.json \\\n",
        "  --validation_file valid_data.json \\\n",
        "  --model_name_or_path hfl/chinese-roberta-wwm-ext \\\n",
        "  --tokenizer_name hfl/chinese-roberta-wwm-ext \\\n",
        "  --output_dir ./output \\\n",
        "  --per_device_train_batch_size 4 \\\n",
        "  --per_device_eval_batch_size 4 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --max_seq_length 512 \\\n",
        "  --pad_to_max_length \\\n",
        "  --lr_scheduler_type cosine \\\n",
        "  --seed 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "670PtSW7ldTE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "250e6917-cb02-490a-dcbe-3e2c184d10b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: output/ (stored 0%)\n",
            "  adding: output/config.json (deflated 54%)\n",
            "  adding: output/model.safetensors (deflated 7%)\n",
            "  adding: output/tokenizer.json (deflated 75%)\n",
            "  adding: output/special_tokens_map.json (deflated 80%)\n",
            "  adding: output/vocab.txt (deflated 48%)\n",
            "  adding: output/tokenizer_config.json (deflated 75%)\n",
            "  adding: output/all_results.json (stored 0%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r output.zip ./output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OjhuhrZutxIq"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# files.download('./output.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "hpE1-rPSE4P5"
      },
      "outputs": [],
      "source": [
        "def transform_to_testing_format(df, contexts):\n",
        "    formatted_data = []\n",
        "\n",
        "    # Loop over each row in the DataFrame\n",
        "    for index, row in df.iterrows():\n",
        "        question = row['question']\n",
        "        paragraphs = row['paragraphs']  # These are indices of contexts\n",
        "\n",
        "        # Create the format\n",
        "        entry = {\n",
        "            \"id\": row['id'],\n",
        "            \"sent1\": question,\n",
        "            \"ending0\": contexts[paragraphs[0]],\n",
        "            \"ending1\": contexts[paragraphs[1]],\n",
        "            \"ending2\": contexts[paragraphs[2]],\n",
        "            \"ending3\": contexts[paragraphs[3]],\n",
        "        }\n",
        "\n",
        "        # Append the formatted entry\n",
        "        formatted_data.append(entry)\n",
        "\n",
        "    return formatted_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_json('test.json')"
      ],
      "metadata": {
        "id": "tJl-gSW6MbgU"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "B3Iry7M2D0Hh"
      },
      "outputs": [],
      "source": [
        "test_data = transform_to_testing_format(test, contexts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "mRX-c6GZEU4H"
      },
      "outputs": [],
      "source": [
        "with open('test_data.json', 'w') as test_data_json:\n",
        "    json.dump(test_data, test_data_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2DXTYCKIbre",
        "outputId": "1938cc43-aaac-4d68-c9c3-8de4a3720145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1NKYF5YR9RYJFXFehrHBtT2o5CsloEwOg\n",
            "From (redirected): https://drive.google.com/uc?id=1NKYF5YR9RYJFXFehrHBtT2o5CsloEwOg&confirm=t&uuid=a5c07fdd-ddcc-43a4-bbf9-77d8d7b96dca\n",
            "To: /content/multiple-choice-fine-tune-96.zip\n",
            "100% 380M/380M [00:08<00:00, 45.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1NKYF5YR9RYJFXFehrHBtT2o5CsloEwOg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzDTmfKgJmgO",
        "outputId": "76ddfe63-957d-4ebb-bd47-4fd01f7ce9c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ./multiple-choice-fine-tune-96.zip\n",
            "   creating: multiple-choice-fine-tune-96/\n",
            "  inflating: __MACOSX/._multiple-choice-fine-tune-96  \n",
            "  inflating: multiple-choice-fine-tune-96/model.safetensors  \n",
            "  inflating: __MACOSX/multiple-choice-fine-tune-96/._model.safetensors  \n",
            "  inflating: multiple-choice-fine-tune-96/tokenizer_config.json  \n",
            "  inflating: __MACOSX/multiple-choice-fine-tune-96/._tokenizer_config.json  \n",
            "  inflating: multiple-choice-fine-tune-96/special_tokens_map.json  \n",
            "  inflating: __MACOSX/multiple-choice-fine-tune-96/._special_tokens_map.json  \n",
            "  inflating: multiple-choice-fine-tune-96/config.json  \n",
            "  inflating: __MACOSX/multiple-choice-fine-tune-96/._config.json  \n",
            "  inflating: multiple-choice-fine-tune-96/tokenizer.json  \n",
            "  inflating: __MACOSX/multiple-choice-fine-tune-96/._tokenizer.json  \n",
            "  inflating: multiple-choice-fine-tune-96/vocab.txt  \n",
            "  inflating: __MACOSX/multiple-choice-fine-tune-96/._vocab.txt  \n",
            "  inflating: multiple-choice-fine-tune-96/all_results.json  \n",
            "  inflating: __MACOSX/multiple-choice-fine-tune-96/._all_results.json  \n"
          ]
        }
      ],
      "source": [
        "!unzip ./multiple-choice-fine-tune-96.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "-1-PfdcHPspe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpyencoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMIjEM4NNrZy",
        "outputId": "2c0e2763-22fb-492c-a3ff-70fe6f19072a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpyencoder\n",
            "  Downloading numpyencoder-0.3.0-py3-none-any.whl.metadata (871 bytes)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from numpyencoder) (1.26.4)\n",
            "Downloading numpyencoder-0.3.0-py3-none-any.whl (3.0 kB)\n",
            "Installing collected packages: numpyencoder\n",
            "Successfully installed numpyencoder-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "ZMDsycotFI5m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13f289ce-f04f-41b2-efbd-def04223056b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-10-08 12:59:50.822064: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-10-08 12:59:50.840090: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-08 12:59:50.861436: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-08 12:59:50.867859: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-08 12:59:50.883365: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-08 12:59:52.068604: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Predictions saved to predictions.json\\\n"
          ]
        }
      ],
      "source": [
        "!python multiple_choice_inference.py \\\n",
        "    --test_file test_data.json \\\n",
        "    --model_dir ./multiple-choice-fine-tune-96 \\\n",
        "    --output_file predictions.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "DxKnbWhzJwAX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}